{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAPTER 11**\n",
        "# **Training Deep Neural Networks**\n",
        "\n",
        "This subchapter discusses one of the main difficulties in training deep neural networks: the vanishing and exploding gradients problem. During backpropagation, gradients are propagated backward from the output layer to the earlier layers. If gradients become too small, learning slows down or stops (vanishing gradients). If gradients become too large, weights grow uncontrollably (exploding gradients).\n",
        "These problems were especially severe in deep networks using sigmoid or tanh activation functions, which compress inputs into a small range. As a result, early layers learn extremely slowly compared to later layers.\n",
        "This issue was one of the main reasons deep neural networks were difficult to train before 2010.\n"
      ],
      "metadata": {
        "id": "gdK9phBVrd6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Xavier and He Initialization**\n",
        "\n",
        "To reduce vanishing and exploding gradients, better weight initialization strategies were introduced.\n",
        "Xavier (Glorot) initialization sets initial weights so that the variance of activations remains stable across layers. It works well with sigmoid and tanh activations.\n",
        "He initialization is a variant optimized for ReLU activation functions. It uses a higher variance to account for the fact that ReLU outputs zero for half of its inputs.\n",
        "Keras automatically uses these initializers depending on the activation function.\n"
      ],
      "metadata": {
        "id": "11IaGjULrkEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nonsaturating Activation Functions**\n",
        "\n",
        "This subchapter explains how choosing the right activation function helps deep networks train faster and more reliably.\n",
        "•\tReLU is the most widely used activation function because it reduces vanishing gradients.\n",
        "•\tLeaky ReLU allows a small slope for negative inputs to avoid “dead neurons”.\n",
        "•\tELU (Exponential Linear Unit) improves convergence speed and robustness to noise.\n",
        "•\tSELU enables self-normalizing neural networks when used with specific initializations.\n"
      ],
      "metadata": {
        "id": "2VoMNWhMro59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\", input_shape=(20,)),  # contoh input 20 fitur\n",
        "    keras.layers.Dense(32),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")  # misal 10 kelas output\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eBSJXzHrvC_",
        "outputId": "2a600854-6f5f-4658-b82d-4cb824bd8417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = keras.layers.Dense(10, activation=\"selu\",\n",
        "kernel_initializer=\"lecun_normal\")"
      ],
      "metadata": {
        "id": "pBqLZUplr3E_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**\n",
        "\n",
        "Batch Normalization (BN) addresses the problem of internal covariate shift by normalizing inputs of each layer during training. This stabilizes learning, allows higher learning rates, and reduces sensitivity to initialization.\n",
        "BN also acts as a regularizer, often reducing the need for dropout.\n",
        "Batch normalization layers typically come before or after activation functions.\n"
      ],
      "metadata": {
        "id": "j97dwVffsCV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR1RodH5sG4z",
        "outputId": "d73fdd38-bab0-497e-a6dc-e33279100b14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "nxfrBtjZsQz2",
        "outputId": "d37d1dbe-b758-4e0f-8faf-fec138d369a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYY_1sMMsT2d",
        "outputId": "da0a1235-a1ca-475f-d131-520c9604cad3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gamma', True),\n",
              " ('beta', True),\n",
              " ('moving_mean', False),\n",
              " ('moving_variance', False)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn_layer = model.layers[1]\n",
        "\n",
        "# Moving mean dan moving variance\n",
        "moving_mean = bn_layer.moving_mean\n",
        "moving_var = bn_layer.moving_variance\n",
        "\n",
        "print(\"Moving mean:\", moving_mean.numpy())\n",
        "print(\"Moving variance:\", moving_var.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uxcdRLDsWuE",
        "outputId": "96e77b9e-be4f-45bc-d5c8-0975c3bd2f76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving mean: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Moving variance: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "keras.layers.Flatten(input_shape=[28, 28]),\n",
        "keras.layers.BatchNormalization(),\n",
        "keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "keras.layers.BatchNormalization(),\n",
        "keras.layers.Activation(\"elu\"),\n",
        "keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "keras.layers.BatchNormalization(),\n",
        "keras.layers.Activation(\"elu\"),\n",
        "keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "Ljtti-cLsj1R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization(keras.layers.Layer):\n",
        "    [...]\n",
        "def call(self, inputs, training=None):\n",
        "    [...]"
      ],
      "metadata": {
        "id": "YU_CUng6soN_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Clipping**\n",
        "\n",
        "Gradient clipping limits the magnitude of gradients during backpropagation to prevent exploding gradients. This is especially useful for recurrent neural networks but can also help deep feedforward networks.\n",
        "Keras supports gradient clipping via the optimizer.\n"
      ],
      "metadata": {
        "id": "-_bZLhXFswhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)"
      ],
      "metadata": {
        "id": "KHoBFJASs0U5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reusing Pretrained Layers**\n",
        "\n",
        "Transfer learning allows reuse of pretrained models or layers, which significantly reduces training time and improves performance when data is limited.\n",
        "Typical steps:\n",
        "1.\tLoad a pretrained model\n",
        "2.\tFreeze some layers\n",
        "3.\tTrain remaining layers\n",
        "4.\tOptionally fine-tune frozen layers with a lower learning rate\n"
      ],
      "metadata": {
        "id": "sY-N7AFrs4kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faster Optimizers**\n",
        "\n",
        "This section introduces advanced optimization algorithms that converge faster than plain Gradient Descent.\n",
        "Common optimizers:\n",
        "•\tMomentum\n",
        "•\tNesterov Accelerated Gradient\n",
        "•\tRMSProp\n",
        "•\tAdam\n",
        "•\tAdamax\n",
        "•\tNadam\n"
      ],
      "metadata": {
        "id": "_6dgo8ZWtB2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "yHCfcFDotE9s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n"
      ],
      "metadata": {
        "id": "r78ZJFUst-_H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.001, rho=0.9)\n"
      ],
      "metadata": {
        "id": "eUqK1zDnuUMm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n"
      ],
      "metadata": {
        "id": "n-vy_4Sluked"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate Scheduling**\n",
        "\n",
        "Choosing a fixed learning rate is rarely optimal. Learning rate scheduling dynamically adjusts the learning rate during training to improve convergence.\n",
        "Techniques include:\n",
        "•\tStep decay\n",
        "•\tExponential decay\n",
        "•\tPower scheduling\n",
        "•\tPerformance-based scheduling\n"
      ],
      "metadata": {
        "id": "YrICiHSYulrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n"
      ],
      "metadata": {
        "id": "zIyYgBV2ura0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)"
      ],
      "metadata": {
        "id": "Csserc3quxPX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1**(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
      ],
      "metadata": {
        "id": "VL0uhZ-Cu1uH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "EwtNiFUVu_cl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
      ],
      "metadata": {
        "id": "DBClm1WHv04w"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "3eN3DRduvZLj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.Input(shape=(X_train_scaled.shape[1],)),   # input layer\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')     # output 3 kelas\n",
        "])"
      ],
      "metadata": {
        "id": "YAPwb1jxv86O"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "GotjtX3Rv-_y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_decay_fn(epoch, lr):\n",
        "    k = 0.1\n",
        "    return float(lr * np.exp(-k * epoch))\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n"
      ],
      "metadata": {
        "id": "kp9XhZWJwBfB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=20,\n",
        "    validation_data=(X_valid_scaled, y_valid),\n",
        "    callbacks=[lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvQA2sWJwEmm",
        "outputId": "87eca783-c349-46f0-835e-51549b8ef88a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.2934 - loss: 1.1930 - val_accuracy: 0.3636 - val_loss: 1.0663 - learning_rate: 0.0100\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4696 - loss: 0.9867 - val_accuracy: 0.6364 - val_loss: 0.8560 - learning_rate: 0.0090\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6472 - loss: 0.8150 - val_accuracy: 0.6818 - val_loss: 0.7387 - learning_rate: 0.0074\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6774 - loss: 0.7034 - val_accuracy: 0.7727 - val_loss: 0.6649 - learning_rate: 0.0055\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7368 - loss: 0.6085 - val_accuracy: 0.7727 - val_loss: 0.6160 - learning_rate: 0.0037\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7551 - loss: 0.5876 - val_accuracy: 0.7727 - val_loss: 0.5829 - learning_rate: 0.0022\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7607 - loss: 0.5532 - val_accuracy: 0.7727 - val_loss: 0.5611 - learning_rate: 0.0012\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7738 - loss: 0.5617 - val_accuracy: 0.7727 - val_loss: 0.5469 - learning_rate: 6.0810e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7704 - loss: 0.5241 - val_accuracy: 0.7727 - val_loss: 0.5379 - learning_rate: 2.7324e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7731 - loss: 0.5307 - val_accuracy: 0.7727 - val_loss: 0.5322 - learning_rate: 1.1109e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7759 - loss: 0.5424 - val_accuracy: 0.7727 - val_loss: 0.5284 - learning_rate: 4.0868e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8009 - loss: 0.5189 - val_accuracy: 0.7727 - val_loss: 0.5259 - learning_rate: 1.3604e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8165 - loss: 0.5208 - val_accuracy: 0.7727 - val_loss: 0.5243 - learning_rate: 4.0973e-06\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7999 - loss: 0.4934 - val_accuracy: 0.7727 - val_loss: 0.5233 - learning_rate: 1.1167e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7915 - loss: 0.5243 - val_accuracy: 0.7727 - val_loss: 0.5226 - learning_rate: 2.7536e-07\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8453 - loss: 0.4849 - val_accuracy: 0.7727 - val_loss: 0.5222 - learning_rate: 6.1442e-08\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8224 - loss: 0.5031 - val_accuracy: 0.7727 - val_loss: 0.5219 - learning_rate: 1.2405e-08\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8099 - loss: 0.4987 - val_accuracy: 0.7727 - val_loss: 0.5217 - learning_rate: 2.2662e-09\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7787 - loss: 0.5318 - val_accuracy: 0.7727 - val_loss: 0.5215 - learning_rate: 3.7460e-10\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8141 - loss: 0.4864 - val_accuracy: 0.7727 - val_loss: 0.5215 - learning_rate: 5.6028e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_test, acc_test = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"Test accuracy:\", acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYVhk5T4wGj-",
        "outputId": "7ab29b6e-f1ae-430e-a07f-3d5fdbd78b59"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6957 - loss: 0.6062\n",
            "Test accuracy: 0.695652186870575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test_scaled[:3]\n",
        "y_pred = model.predict(X_new)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "print(\"Predicted classes:\", y_pred_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYcpkPuUwHXF",
        "outputId": "8ddc1013-a4d5-48b6-bf5b-2eade301b0ba"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "Predicted classes: [0 0 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_decay_fn(epoch, lr):\n",
        "    return lr * 0.1**(1 / 20)"
      ],
      "metadata": {
        "id": "juN03-v2wKap"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 15:\n",
        "        return 0.005\n",
        "    else:\n",
        "        return 0.001"
      ],
      "metadata": {
        "id": "bMTltScywN-a"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
      ],
      "metadata": {
        "id": "yZuvYPdHwaVO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
        "optimizer = keras.optimizers.SGD(learning_rate)"
      ],
      "metadata": {
        "id": "q1ahgy3PwddT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avoiding Overfitting Through Regularization**\n",
        "\n",
        "Deep networks are prone to overfitting due to their large number of parameters.\n",
        "Regularization techniques discussed:\n",
        "•\tL1 and L2 regularization\n",
        "•\tDropout\n",
        "•\tMax-norm constraints\n",
        "•\tEarly stopping\n"
      ],
      "metadata": {
        "id": "tMP9Sxe9wihY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = keras.layers.Dense(100, activation=\"elu\",\n",
        "kernel_initializer=\"he_normal\",\n",
        "\n",
        "kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "metadata": {
        "id": "_t6j5CLKwlBa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "activation=\"elu\",\n",
        "kernel_initializer=\"he_normal\",\n",
        "kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "keras.layers.Flatten(input_shape=[28, 28]),\n",
        "RegularizedDense(300),\n",
        "RegularizedDense(100),\n",
        "RegularizedDense(10, activation=\"softmax\",\n",
        "kernel_initializer=\"glorot_uniform\")\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_S95WK0woFI",
        "outputId": "f91525b1-8779-4c3f-b6af-11b1a9481ee6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "keras.layers.Flatten(input_shape=[28, 28]),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "yn3VbO7UwxHN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "jn0WHNyZwyDL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data          # 4 fitur\n",
        "y = iris.target.reshape(-1, 1)  # 0,1,2"
      ],
      "metadata": {
        "id": "F-6IeMifxH-Q"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y)  # shape (150, 3)\n"
      ],
      "metadata": {
        "id": "aZnVYM0fxJyL"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "wU6Cs_JMxLin"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "sCsgT4JVxWTf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.Input(shape=(4,)),             # 4 fitur input\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(3, activation='softmax')  # 3 kelas\n",
        "])"
      ],
      "metadata": {
        "id": "FQARE3XGxX6W"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "92z_khXaxZ_v"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d3S8EBaxb8i",
        "outputId": "e77cb2c6-d7e9-4cae-becd-1adec992f7b2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.2818 - loss: 1.1599 - val_accuracy: 0.5000 - val_loss: 0.9817\n",
            "Epoch 2/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2439 - loss: 1.1192 - val_accuracy: 0.5000 - val_loss: 0.9522\n",
            "Epoch 3/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2092 - loss: 1.0697 - val_accuracy: 0.5000 - val_loss: 0.9261\n",
            "Epoch 4/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3077 - loss: 1.0620 - val_accuracy: 0.5000 - val_loss: 0.8990\n",
            "Epoch 5/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2533 - loss: 1.0909 - val_accuracy: 0.5417 - val_loss: 0.8752\n",
            "Epoch 6/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3290 - loss: 1.0235 - val_accuracy: 0.6667 - val_loss: 0.8512\n",
            "Epoch 7/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3659 - loss: 1.0233 - val_accuracy: 0.7083 - val_loss: 0.8281\n",
            "Epoch 8/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3923 - loss: 0.9458 - val_accuracy: 0.7500 - val_loss: 0.8050\n",
            "Epoch 9/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5467 - loss: 0.8783 - val_accuracy: 0.7500 - val_loss: 0.7815\n",
            "Epoch 10/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5568 - loss: 0.9056 - val_accuracy: 0.7917 - val_loss: 0.7628\n",
            "Epoch 11/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6110 - loss: 0.9299 - val_accuracy: 0.8750 - val_loss: 0.7427\n",
            "Epoch 12/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6190 - loss: 0.8904 - val_accuracy: 0.8750 - val_loss: 0.7229\n",
            "Epoch 13/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6571 - loss: 0.8223 - val_accuracy: 0.8750 - val_loss: 0.7026\n",
            "Epoch 14/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6999 - loss: 0.8054 - val_accuracy: 0.8750 - val_loss: 0.6839\n",
            "Epoch 15/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7405 - loss: 0.8243 - val_accuracy: 0.8750 - val_loss: 0.6634\n",
            "Epoch 16/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.6985 - loss: 0.7729 - val_accuracy: 0.9167 - val_loss: 0.6456\n",
            "Epoch 17/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6705 - loss: 0.7950 - val_accuracy: 0.9167 - val_loss: 0.6256\n",
            "Epoch 18/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7551 - loss: 0.7454 - val_accuracy: 0.9167 - val_loss: 0.6054\n",
            "Epoch 19/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7810 - loss: 0.6890 - val_accuracy: 0.9167 - val_loss: 0.5860\n",
            "Epoch 20/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8262 - loss: 0.6559 - val_accuracy: 0.9583 - val_loss: 0.5676\n",
            "Epoch 21/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7958 - loss: 0.6562 - val_accuracy: 0.9583 - val_loss: 0.5511\n",
            "Epoch 22/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8512 - loss: 0.5923 - val_accuracy: 0.9583 - val_loss: 0.5348\n",
            "Epoch 23/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8582 - loss: 0.5639 - val_accuracy: 0.9583 - val_loss: 0.5182\n",
            "Epoch 24/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7683 - loss: 0.5879 - val_accuracy: 0.9583 - val_loss: 0.5036\n",
            "Epoch 25/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7964 - loss: 0.5737 - val_accuracy: 0.9167 - val_loss: 0.4876\n",
            "Epoch 26/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8899 - loss: 0.4787 - val_accuracy: 0.9167 - val_loss: 0.4746\n",
            "Epoch 27/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8101 - loss: 0.5262 - val_accuracy: 0.9167 - val_loss: 0.4634\n",
            "Epoch 28/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8094 - loss: 0.4986 - val_accuracy: 0.9167 - val_loss: 0.4496\n",
            "Epoch 29/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8449 - loss: 0.5447 - val_accuracy: 0.9167 - val_loss: 0.4363\n",
            "Epoch 30/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8740 - loss: 0.4826 - val_accuracy: 0.9167 - val_loss: 0.4288\n",
            "Epoch 31/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7963 - loss: 0.5676 - val_accuracy: 0.9167 - val_loss: 0.4146\n",
            "Epoch 32/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7990 - loss: 0.4448 - val_accuracy: 0.9167 - val_loss: 0.4046\n",
            "Epoch 33/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8177 - loss: 0.4870 - val_accuracy: 0.9167 - val_loss: 0.3937\n",
            "Epoch 34/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8671 - loss: 0.4546 - val_accuracy: 0.9167 - val_loss: 0.3808\n",
            "Epoch 35/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8879 - loss: 0.4117 - val_accuracy: 0.9167 - val_loss: 0.3678\n",
            "Epoch 36/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8235 - loss: 0.4536 - val_accuracy: 0.9167 - val_loss: 0.3580\n",
            "Epoch 37/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8149 - loss: 0.4448 - val_accuracy: 0.9167 - val_loss: 0.3456\n",
            "Epoch 38/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8121 - loss: 0.4188 - val_accuracy: 0.9167 - val_loss: 0.3372\n",
            "Epoch 39/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8493 - loss: 0.4265 - val_accuracy: 0.9167 - val_loss: 0.3302\n",
            "Epoch 40/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9143 - loss: 0.3419 - val_accuracy: 0.9167 - val_loss: 0.3248\n",
            "Epoch 41/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8713 - loss: 0.3764 - val_accuracy: 0.9167 - val_loss: 0.3234\n",
            "Epoch 42/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8451 - loss: 0.4264 - val_accuracy: 0.9167 - val_loss: 0.3214\n",
            "Epoch 43/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8619 - loss: 0.3427 - val_accuracy: 0.9167 - val_loss: 0.3205\n",
            "Epoch 44/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8257 - loss: 0.4535 - val_accuracy: 0.9167 - val_loss: 0.3122\n",
            "Epoch 45/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9086 - loss: 0.3528 - val_accuracy: 0.9167 - val_loss: 0.3050\n",
            "Epoch 46/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8680 - loss: 0.4356 - val_accuracy: 0.9167 - val_loss: 0.2939\n",
            "Epoch 47/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9156 - loss: 0.3398 - val_accuracy: 0.9167 - val_loss: 0.2850\n",
            "Epoch 48/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8396 - loss: 0.4004 - val_accuracy: 0.9167 - val_loss: 0.2859\n",
            "Epoch 49/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8887 - loss: 0.3060 - val_accuracy: 0.9167 - val_loss: 0.2828\n",
            "Epoch 50/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9222 - loss: 0.3368 - val_accuracy: 0.9167 - val_loss: 0.2778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mc_dropout(model, X, n_iter=100):\n",
        "    preds = np.stack([model(X, training=True).numpy() for _ in range(n_iter)])\n",
        "    return preds.mean(axis=0)"
      ],
      "metadata": {
        "id": "0aaNu0ebxecH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = predict_mc_dropout(model, X_test_scaled, n_iter=100)"
      ],
      "metadata": {
        "id": "yJHKPBAgxgtb"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape probabilitas:\", y_proba.shape)\n",
        "print(\"Contoh probabilitas untuk 5 sampel pertama:\\n\", y_proba[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjq34P1xiVR",
        "outputId": "8d94668a-afa1-4474-fba5-8977bcf10f03"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape probabilitas: (30, 3)\n",
            "Contoh probabilitas untuk 5 sampel pertama:\n",
            " [[4.0260687e-02 6.7122465e-01 2.8851438e-01]\n",
            " [9.2543596e-01 4.1811489e-02 3.2752682e-02]\n",
            " [6.0832663e-04 1.3655116e-01 8.6284041e-01]\n",
            " [3.1433925e-02 5.2417159e-01 4.4439453e-01]\n",
            " [1.9243708e-02 5.3971273e-01 4.4104356e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(model.predict(X_test_scaled[:1]), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRukIUJ5xm1k",
        "outputId": "c2cb432d-12ed-42ff-b42e-638181592dfd"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02, 0.7 , 0.28]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(y_proba[:1], 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG4ABMShxnkK",
        "outputId": "a462a154-f90b-4aac-a63a-b52e829013f7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04, 0.67, 0.29]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary and Practical Guidelines**\n",
        "\n",
        "The chapter concludes with practical advice for training deep neural networks effectively:\n",
        "•\tUse ReLU or its variants\n",
        "•\tApply He initialization\n",
        "•\tPrefer Batch Normalization\n",
        "•\tUse adaptive optimizers like Adam\n",
        "•\tRegularize using dropout or early stopping\n",
        "•\tTune learning rates carefully\n",
        "These techniques collectively make training deep neural networks faster, more stable, and more reliable.\n"
      ],
      "metadata": {
        "id": "L23mCgSox9H4"
      }
    }
  ]
}