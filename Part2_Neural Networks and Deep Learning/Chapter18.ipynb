{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 18 **\n",
        "# **Reinforcement Learning**\n",
        "\n",
        "**Introduction to Reinforcement Learning**\n",
        "\n",
        "This subchapter introduces Reinforcement Learning (RL) as a learning paradigm where an agent learns by interacting with an environment. Unlike supervised learning, RL does not rely on labeled datasets. Instead, the agent receives rewards or penalties based on the actions it takes.\n",
        "\n",
        "The objective of the agent is to learn a policy that maximizes the cumulative reward over time. At each step, the agent observes the current state, chooses an action, receives a reward, and transitions to a new state. This interaction loop continues until a terminal condition is reached.\n",
        "\n",
        "Key challenges in RL include:\n",
        "\n",
        "Exploration vs. exploitation trade-off\n",
        "\n",
        "Delayed rewards\n",
        "\n",
        "Large or continuous state spaces\n",
        "\n",
        "RL is widely used in robotics, game playing (e.g., AlphaGo), recommendation systems, and autonomous systems."
      ],
      "metadata": {
        "id": "AvlpVf6KgC-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy Search**\n",
        "\n",
        "This section explains policy search methods, where the policy is directly optimized instead of learning value functions.\n",
        "\n",
        "A policy can be:\n",
        "\n",
        "A simple rule-based system\n",
        "\n",
        "A parameterized function (e.g., neural network)\n",
        "\n",
        "The agent evaluates a policy by running it multiple times and measuring the average reward, then updates the policy parameters to improve performance."
      ],
      "metadata": {
        "id": "8ZmjV2tFgLIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy(obs, theta):\n",
        "    return 0 if np.dot(obs, theta) < 0 else 1\n"
      ],
      "metadata": {
        "id": "uFpEubUKgPbn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to OpenAI Gym**\n",
        "\n",
        "OpenAI Gym provides a standardized environment interface for RL experiments. It allows agents to interact with simulated environments using a simple API."
      ],
      "metadata": {
        "id": "x9DI9c6TgU3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Policies**\n",
        "\n",
        "This subchapter introduces neural networks as policies, enabling the agent to handle complex and high-dimensional inputs.\n",
        "\n",
        "A neural network outputs action probabilities or action values, which are then sampled or selected greedily."
      ],
      "metadata": {
        "id": "5lKuS2rBhTNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 4\n",
        "n_hidden = 4\n",
        "n_outputs = 1\n",
        "\n",
        "initializer = tf.keras.initializers.VarianceScaling()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=initializer),\n",
        "    tf.keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=initializer),\n",
        "    tf.keras.layers.Dense(n_outputs, activation=\"sigmoid\", kernel_initializer=initializer)\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8epcUV0hUha",
        "outputId": "517e2fda-f6c5-4acf-b3c5-1f4d418326bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Actions: The Policy Gradient**\n",
        "\n",
        "This section introduces the policy gradient approach, where gradients are computed to adjust the policy in the direction that increases expected rewards.\n",
        "\n",
        "The key idea is:\n",
        "\n",
        "Actions leading to higher rewards are reinforced\n",
        "\n",
        "Actions leading to lower rewards are discouraged\n",
        "\n",
        "The algorithm samples trajectories, computes rewards, and updates parameters accordingly."
      ],
      "metadata": {
        "id": "vI9iH2unhaJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "41j3hZOshcB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Policy Gradients**\n",
        "\n",
        "This subchapter explains how to implement the REINFORCE algorithm."
      ],
      "metadata": {
        "id": "T2mw784HhddY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.zeros_like(rewards)\n",
        "    cumulative = 0\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        cumulative = rewards[step] + cumulative * discount_rate\n",
        "        discounted[step] = cumulative\n",
        "    return discounted\n"
      ],
      "metadata": {
        "id": "qUZ9dakshfoQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Policy**\n",
        "\n",
        "During training:\n",
        "\n",
        "Multiple episodes are run\n",
        "\n",
        "Rewards are collected\n",
        "\n",
        "Gradients are computed\n",
        "\n",
        "Model parameters are updated"
      ],
      "metadata": {
        "id": "rZwKbUrghh5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "X = tf.random.normal([32, 5])\n",
        "y = tf.random.normal([32, 1])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y_pred = model(X, training=True)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.mse(y, y_pred))\n",
        "\n",
        "grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "print(\"Loss:\", loss.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpqwF6A5hj_v",
        "outputId": "e1d56281-cf42-4475-a7c9-e833b5759201"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.580145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages and Limitations of Policy Gradients**\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Works well with continuous action spaces\n",
        "\n",
        "Directly optimizes the policy\n",
        "\n",
        "Can learn stochastic behaviors\n",
        "\n",
        "Limitations:\n",
        "\n",
        "High variance in gradient estimates\n",
        "\n",
        "Requires many episodes\n",
        "\n",
        "Sensitive to hyperparameters"
      ],
      "metadata": {
        "id": "82QuU82lihZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Reinforcement Learning Concepts**\n",
        "\n",
        "This chapter provides a foundational understanding of reinforcement learning, focusing on:\n",
        "\n",
        "Agent-environment interaction\n",
        "\n",
        "Policy-based learning\n",
        "\n",
        "Policy gradients and neural policies\n",
        "\n",
        "Practical implementation using OpenAI Gym and TensorFlow\n",
        "\n",
        "These concepts prepare the reader for more advanced algorithms such as Actor-Critic, Deep Q-Learning, and Proximal Policy Optimization (PPO) in later chapters."
      ],
      "metadata": {
        "id": "f4XeJmtBioMH"
      }
    }
  ]
}