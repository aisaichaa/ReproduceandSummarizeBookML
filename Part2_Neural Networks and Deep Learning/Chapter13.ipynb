{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAPTER 13**\n",
        "# **Loading and Preprocessing Data with TensorFlow**\n",
        "\n",
        "**The Data API**\n",
        "\n",
        "This subchapter introduces the TensorFlow Data API, which is designed to build efficient, scalable, and flexible input pipelines. The Data API allows datasets to be streamed from disk, transformed, shuffled, batched, and prefetched, enabling models to train efficiently even with very large datasets that do not fit into memory.\n",
        "A tf.data.Dataset represents a sequence of elements, where each element can be a tensor, tuple, or dictionary of tensors. The API emphasizes lazy evaluation, meaning transformations are only executed when data is actually consumed.\n"
      ],
      "metadata": {
        "id": "tOb12-TA5ybL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Datasets from Tensors**\n",
        "\n",
        "This section explains how to create datasets directly from tensors or NumPy arrays. This approach is suitable for small to medium-sized datasets that fit entirely in memory.\n"
      ],
      "metadata": {
        "id": "4CsWNUJj558y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.range(10)  # buat tensor data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqhocFp16CQ_",
        "outputId": "87cd06a2-9668-4f05-9f12-6f2ed83a05bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSrEifKM6eop",
        "outputId": "0d476d0e-53c4-45e3-91bb-3e49935fccf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chaining Transformations**\n",
        "\n",
        "The Data API supports chaining multiple transformations to build complex pipelines. Common transformations include:\n",
        "•\trepeat()\n",
        "•\tbatch()\n",
        "•\tmap()\n",
        "•\tshuffle()\n",
        "•\tprefetch()\n"
      ],
      "metadata": {
        "id": "fCzzvFut6uCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldWx81x-6s2Y",
        "outputId": "1b26f32e-996e-4bdf-ff76-abd59679c145"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda x: x * 2)"
      ],
      "metadata": {
        "id": "DJjqEWJp6fGk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.apply(tf.data.experimental.unbatch())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDiY9T5X6kGB",
        "outputId": "f0d85c79-8f08-4681-f647-fd95a5221ff7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-643490874.py:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.unbatch()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda x: x < 10)"
      ],
      "metadata": {
        "id": "R39Ay2BY6-WL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvU_GtC7Azm",
        "outputId": "6f28ca8b-b574-4936-9fa4-34a8e2329357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffling the Data**\n",
        "\n",
        "Shuffling is critical for training neural networks to avoid learning artifacts from data ordering. TensorFlow shuffles data using a fixed-size buffer, ensuring randomness without loading the entire dataset into memory.\n"
      ],
      "metadata": {
        "id": "uLr9oJTT7Ghu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vb4oQsq7JT6",
        "outputId": "f83b67fb-87ea-4f16-da63-a27fc5434a4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing the Data**\n",
        "\n",
        "This subchapter focuses on applying preprocessing operations directly inside the input pipeline using the map() method. Preprocessing may include scaling, normalization, feature engineering, or data augmentation."
      ],
      "metadata": {
        "id": "2uVmoFs37WF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Contoh untuk 8 fitur\n",
        "X_mean = tf.constant([0.5, 0.4, 0.3, 0.2, 0.1, 0.0, -0.1, -0.2], dtype=tf.float32)\n",
        "X_std  = tf.constant([1.0, 1.1, 0.9, 1.2, 1.0, 1.3, 0.8, 1.0], dtype=tf.float32)\n",
        "\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [0.]  # default value untuk CSV\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y\n"
      ],
      "metadata": {
        "id": "1H-FDQJn7SD7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUi0KYwL7q0P",
        "outputId": "5cfd9434-2b34-4542-e9be-bf579736f627"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 3.7083001e+00,  3.9636360e+01,  5.5813336e+00,  5.9758335e-01,\n",
              "         8.4590002e+02,  1.7976923e+00,  4.6962498e+01, -1.2200000e+02],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Putting It All Together**\n",
        "\n",
        "This section combines multiple transformations into a single pipeline. The order of operations is important for efficiency.\n",
        "Typical pipeline structure:\n",
        "1.\tShuffle\n",
        "2.\tRepeat\n",
        "3.\tMap (preprocess)\n",
        "4.\tBatch\n",
        "5.\tPrefetch\n"
      ],
      "metadata": {
        "id": "SoYTo59G7sTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_reader_dataset(\n",
        "    filepaths,\n",
        "    repeat=1,\n",
        "    n_readers=5,\n",
        "    n_read_threads=None,\n",
        "    shuffle_buffer_size=10000,\n",
        "    n_parse_threads=5,\n",
        "    batch_size=32\n",
        "):\n",
        "    # Buat dataset dari daftar file CSV\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "\n",
        "    # Baca file secara paralel\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),  # skip header\n",
        "        cycle_length=n_readers,\n",
        "        num_parallel_calls=n_read_threads\n",
        "    )\n",
        "\n",
        "    # Parse tiap baris CSV\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "\n",
        "    # Shuffle dan ulang dataset sesuai repeat\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "\n",
        "    # Batch dan prefetch untuk efisiensi\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "prkDoyXI7wLn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prefetching the Data**\n",
        "\n",
        "Prefetching overlaps data preprocessing and model execution, significantly improving performance. While the model is training on one batch, the next batch is prepared in parallel.\n"
      ],
      "metadata": {
        "id": "BZkD_PDC76oQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the Data API with Keras**\n",
        "\n",
        "This subchapter explains how seamlessly tf.data.Dataset integrates with Keras models. Datasets can be passed directly to model.fit(), model.evaluate(), and model.predict()."
      ],
      "metadata": {
        "id": "kWDhsZd28IF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4uQT7pG98arl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = [\"data/train1.csv\", \"data/train2.csv\"]  # ganti dengan path file training\n",
        "valid_filepaths = [\"data/valid.csv\"]                       # ganti dengan path file validasi\n",
        "test_filepaths  = [\"data/test.csv\"]                        # ganti dengan path file test\n"
      ],
      "metadata": {
        "id": "2KI5QOyi8ynB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 8   # jumlah fitur\n",
        "# mean dan std tiap fitur dari training set, bisa dihitung dari numpy\n",
        "X_mean = tf.constant([0.5]*n_inputs, dtype=tf.float32)\n",
        "X_std  = tf.constant([0.2]*n_inputs, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "4ZOWVwS981Kz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(line):\n",
        "    # default value per kolom: semua 0.0\n",
        "    defs = [0.0] * n_inputs + [0.0]  # label terakhir\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    # standardisasi fitur\n",
        "    x = (x - X_mean) / X_std\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "N6C_6f1Q82ve"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),  # skip header\n",
        "        cycle_length=n_readers,\n",
        "        num_parallel_calls=n_read_threads\n",
        "    )\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "    return dataset.batch(batch_size).prefetch(1)\n"
      ],
      "metadata": {
        "id": "uF4QnjgP85WT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Data from Files**\n",
        "\n",
        "TensorFlow supports loading data from various file formats, including:\n",
        "•\tCSV\n",
        "•\tBinary files\n",
        "•\tTFRecord (recommended for large-scale training)\n",
        "This section emphasizes the importance of efficient file formats for performance and portability.\n",
        "\n",
        "**The TFRecord Format**\n",
        "\n",
        "TFRecord is TensorFlows preferred binary format for storing large datasets. It is optimized for sequential reading and works well with distributed systems.\n"
      ],
      "metadata": {
        "id": "jcC91bsw9E5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")\n"
      ],
      "metadata": {
        "id": "es0TtYAg88LX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpRM4v69WwX",
        "outputId": "3f3a49cb-f8e8-492b-e4f9-c528fab05eee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compressed TFRecord Files**\n",
        "\n",
        "It can sometimes be useful to compress your TFRecord files, especially if they need to\n",
        "be loaded via a network connection. You can create a compressed TFRecord file by\n",
        "setting the options argument:"
      ],
      "metadata": {
        "id": "W2DAwjpC9hqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Opsi untuk menulis TFRecord dengan kompresi GZIP\n",
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options=options) as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")\n"
      ],
      "metadata": {
        "id": "zsWDblXG9kF4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
        "compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "KzFmWlUG9tRu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Brief Introduction to Protocol Buffers**\n",
        "\n",
        "This is a portable, extensi‐\n",
        "ble, and efficient binary format developed at Google back in 2001 and made open\n",
        "source in 2008; protobufs are now widely used, in particular in gRPC, Google’s\n",
        "remote procedure call system."
      ],
      "metadata": {
        "id": "HHfEPd4F93F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow Protobufs**\n",
        "\n",
        "The main protobuf typically used in a TFRecord file is the Example protobuf, which\n",
        "represents one instance in a dataset. It contains a list of named features, where each\n",
        "feature can either be a list of byte strings, a list of floats, or a list of integers."
      ],
      "metadata": {
        "id": "7SpNbhSk-8c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example"
      ],
      "metadata": {
        "id": "ozA0mfJR_muo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "_AD03WKc_nlJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfrecord_filename = \"my_contacts.tfrecord\"\n",
        "with tf.io.TFRecordWriter(tfrecord_filename) as f:\n",
        "    f.write(person_example.SerializeToString())\n",
        "\n",
        "print(f\"TFRecord file '{tfrecord_filename}' berhasil dibuat!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coQjyiNu_p-3",
        "outputId": "62616100-ef65-42c3-8489-90e598937903"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecord file 'my_contacts.tfrecord' berhasil dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset([tfrecord_filename])"
      ],
      "metadata": {
        "id": "Zi0K3vLN_r3Y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_example(serialized_example):\n",
        "    # Deskripsikan struktur data\n",
        "    feature_description = {\n",
        "        \"name\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"emails\": tf.io.VarLenFeature(tf.string)\n",
        "    }\n",
        "    parsed = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "    # Untuk VarLenFeature kita konversi ke dense tensor\n",
        "    parsed[\"emails\"] = tf.sparse.to_dense(parsed[\"emails\"])\n",
        "    return parsed\n",
        "\n",
        "parsed_dataset = dataset.map(parse_example)"
      ],
      "metadata": {
        "id": "16odb-Z-_uHI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for record in parsed_dataset:\n",
        "    print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QKa81wW_w4z",
        "outputId": "b9d5ee36-33b5-48de-8c38-3e9dbff0b311"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'emails': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Parsing Examples**\n",
        "\n",
        "To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset\n",
        "once again, and we will parse each Example using tf.io.parse_single_example().\n",
        "This is a TensorFlow operation, so it can be included in a TF Function. It requires at\n",
        "least two arguments: a string scalar tensor containing the serialized data, and a\n",
        "description of each feature."
      ],
      "metadata": {
        "id": "Ufk-QbNo_zOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "W1E5QpD5_6RL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),  # bisa berbeda panjang per contoh\n",
        "}"
      ],
      "metadata": {
        "id": "swe7juZVAD3i"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfrecord_file = \"my_contacts.tfrecord\"\n",
        "dataset = tf.data.TFRecordDataset([tfrecord_file])\n",
        "\n",
        "for serialized_example in dataset:\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "\n",
        "    # Untuk VarLenFeature (emails), bisa akses .values langsung atau konversi ke dense\n",
        "    emails_dense = tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n",
        "\n",
        "    print(\"Name:\", parsed_example[\"name\"].numpy())\n",
        "    print(\"ID:\", parsed_example[\"id\"].numpy())\n",
        "    print(\"Emails:\", emails_dense.numpy())\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4bccxdjAF54",
        "outputId": "dcc8c340-bef7-4173-fe40-bbc3e8a6fade"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: b'Alice'\n",
            "ID: 123\n",
            "Emails: [b'a@b.com' b'c@d.com']\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "batched_dataset = tf.data.TFRecordDataset([tfrecord_file]).batch(batch_size)\n",
        "\n",
        "for serialized_examples in batched_dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_examples, feature_description)\n",
        "\n",
        "    # Convert VarLenFeature to dense tensors\n",
        "    emails_dense_batch = tf.sparse.to_dense(parsed_examples[\"emails\"], default_value=b\"\")\n",
        "\n",
        "    print(\"Batch Names:\", parsed_examples[\"name\"].numpy())\n",
        "    print(\"Batch IDs:\", parsed_examples[\"id\"].numpy())\n",
        "    print(\"Batch Emails:\", emails_dense_batch.numpy())\n",
        "    print(\"===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc4zPc9YAJBn",
        "outputId": "a26c0ae0-4920-4cb3-d75f-521e02b0c1a4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Names: [b'Alice']\n",
            "Batch IDs: [123]\n",
            "Batch Emails: [[b'a@b.com' b'c@d.com']]\n",
            "===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Lists of Lists Using the SequenceExample Protobuf**"
      ],
      "metadata": {
        "id": "JOGI-GgbANLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.train import BytesList, FloatList, Int64List, Feature, Features, FeatureList, FeatureLists, SequenceExample"
      ],
      "metadata": {
        "id": "2y7SkcN1AOJr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_features = Features(feature={\n",
        "    \"user_id\": Feature(int64_list=Int64List(value=[123])),\n",
        "    \"username\": Feature(bytes_list=BytesList(value=[b\"Alice\"]))\n",
        "})\n"
      ],
      "metadata": {
        "id": "WJZsKA4iAcT7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_feature_list = FeatureList(feature=[\n",
        "    Feature(bytes_list=BytesList(value=[b\"Hello\"])),\n",
        "    Feature(bytes_list=BytesList(value=[b\"World\"]))\n",
        "])\n",
        "comments_feature_list = FeatureList(feature=[\n",
        "    Feature(bytes_list=BytesList(value=[b\"Nice\"])),\n",
        "    Feature(bytes_list=BytesList(value=[b\"Post\"]))\n",
        "])\n",
        "\n",
        "sequence_features = FeatureLists(feature_list={\n",
        "    \"content\": content_feature_list,\n",
        "    \"comments\": comments_feature_list\n",
        "})\n",
        "\n",
        "sequence_example = SequenceExample(\n",
        "    context=context_features,\n",
        "    feature_lists=sequence_features\n",
        ")"
      ],
      "metadata": {
        "id": "QHxKyX3rAgUm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.io.TFRecordWriter(\"my_sequence.tfrecord\") as f:\n",
        "    f.write(sequence_example.SerializeToString())"
      ],
      "metadata": {
        "id": "waarwZzwAjYp"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_feature_description = {\n",
        "    \"user_id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "    \"username\": tf.io.FixedLenFeature([], tf.string)\n",
        "}"
      ],
      "metadata": {
        "id": "RC302W9lAlSN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_feature_description = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string),\n",
        "    \"comments\": tf.io.VarLenFeature(tf.string)\n",
        "}\n",
        "\n",
        "dataset = tf.data.TFRecordDataset([\"my_sequence.tfrecord\"])\n",
        "\n",
        "for serialized_seq_example in dataset:\n",
        "    parsed_context, parsed_sequence = tf.io.parse_single_sequence_example(\n",
        "        serialized_seq_example,\n",
        "        context_features=context_feature_description,\n",
        "        sequence_features=sequence_feature_description\n",
        "    )\n",
        "\n",
        "    # Konversi VarLenFeature ke RaggedTensor untuk sequence features\n",
        "    parsed_content = tf.RaggedTensor.from_sparse(parsed_sequence[\"content\"])\n",
        "    parsed_comments = tf.RaggedTensor.from_sparse(parsed_sequence[\"comments\"])\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Context:\", {k: v.numpy() for k, v in parsed_context.items()})\n",
        "    print(\"Content:\", parsed_content.numpy())\n",
        "    print(\"Comments:\", parsed_comments.numpy())\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBojC0zJAnCK",
        "outputId": "7ce4bbb0-ed8a-4f66-dfdd-8d432b094cbf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: {'user_id': np.int64(123), 'username': b'Alice'}\n",
            "Content: [[b'Hello']\n",
            " [b'World']]\n",
            "Comments: [[b'Nice']\n",
            " [b'Post']]\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing the Input Features**\n",
        "\n",
        "Preparing your data for a neural network requires converting all features into numerical features, generally normalizing them, and more. In particular, if your data contains categorical features or text features, they need to be converted to numbers. This\n",
        "can be done ahead of time when preparing your data files, using any tool you like."
      ],
      "metadata": {
        "id": "ZzewxsJ1As63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "LV5q1a28AzQH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.random.rand(100, 8)\n",
        "y_train = np.random.rand(100, 1)\n"
      ],
      "metadata": {
        "id": "3zz024syBNYn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means = np.mean(X_train, axis=0, keepdims=True)\n",
        "stds = np.std(X_train, axis=0, keepdims=True)\n",
        "eps = keras.backend.epsilon()\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks1uL9ZfBBGh",
        "outputId": "fd35867c-414b-4e6b-dc44-ccbfb418f2ca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.2211 - mae: 0.3891\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1889 - mae: 0.3515 \n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1711 - mae: 0.3312 \n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1554 - mae: 0.3123\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1522 - mae: 0.3182 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db534607260>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x14WUt4KBMG4",
        "outputId": "1a90f55d-e1a1-47b1-95fd-de8b47f7ac35"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.1581 - mae: 0.3254 - val_loss: 0.1068 - val_mae: 0.2790\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1588 - mae: 0.3334 - val_loss: 0.1061 - val_mae: 0.2795\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.1420 - mae: 0.3061 - val_loss: 0.1065 - val_mae: 0.2809\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.1304 - mae: 0.2979 - val_loss: 0.1078 - val_mae: 0.2829\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1189 - mae: 0.2849 - val_loss: 0.1093 - val_mae: 0.2846\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1266 - mae: 0.2897 - val_loss: 0.1109 - val_mae: 0.2859\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1180 - mae: 0.2862 - val_loss: 0.1127 - val_mae: 0.2881\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1205 - mae: 0.2884 - val_loss: 0.1144 - val_mae: 0.2912\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1274 - mae: 0.2966 - val_loss: 0.1154 - val_mae: 0.2931\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1086 - mae: 0.2747 - val_loss: 0.1162 - val_mae: 0.2948\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db534770950>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Standardization(keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        eps = keras.backend.epsilon()\n",
        "        return (inputs - self.means_) / (self.stds_ + eps)"
      ],
      "metadata": {
        "id": "TLU4cSmNBX7h"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_layer = Standardization()\n",
        "std_layer.adapt(X_train[:500])"
      ],
      "metadata": {
        "id": "FgWSf7cRBc9P"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = keras.Sequential([\n",
        "    std_layer,\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZn2xvnLBgTe",
        "outputId": "fae4d031-de55-45ea-ba31-0f4b6afa0fab"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step - loss: 0.8099 - mae: 0.7506 - val_loss: 0.6761 - val_mae: 0.6992\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.6896 - mae: 0.7006 - val_loss: 0.6259 - val_mae: 0.6748\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6823 - mae: 0.6887 - val_loss: 0.5805 - val_mae: 0.6511\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.6084 - mae: 0.6537 - val_loss: 0.5399 - val_mae: 0.6283\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.5744 - mae: 0.6328 - val_loss: 0.5029 - val_mae: 0.6056\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4737 - mae: 0.5552 - val_loss: 0.4698 - val_mae: 0.5846\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.3986 - mae: 0.5048 - val_loss: 0.4397 - val_mae: 0.5668\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.4195 - mae: 0.5286 - val_loss: 0.4118 - val_mae: 0.5519\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3881 - mae: 0.5033 - val_loss: 0.3868 - val_mae: 0.5371\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.3940 - mae: 0.5096 - val_loss: 0.3646 - val_mae: 0.5265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db53478e990>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "Chapter 13 focuses on building robust, efficient, and scalable data pipelines using TensorFlow. Key takeaways include:\n",
        "•\tUsing tf.data.Dataset for streaming data\n",
        "•\tApplying transformations lazily\n",
        "•\tLeveraging TFRecord for performance\n",
        "•\tIntegrating preprocessing directly into models\n",
        "This chapter is essential for training deep learning models on real-world, large-scale datasets.\n"
      ],
      "metadata": {
        "id": "qIGeubsYBr9F"
      }
    }
  ]
}