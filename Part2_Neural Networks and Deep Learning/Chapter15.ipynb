{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAPTER 15**\n",
        "# **Processing Sequences Using RNNs and CNNs**\n",
        "\n",
        "**Introduction to Sequential Data and RNNs**\n",
        "\n",
        "This chapter introduces Recurrent Neural Networks (RNNs) as neural network architectures specifically designed to process sequential data. Unlike feedforward neural networks, which assume fixed-size inputs, RNNs can handle sequences of arbitrary length such as time series, text, speech, and audio signals. Humans naturally predict future events by observing sequences, and RNNs attempt to replicate this ability computationally.\n",
        "RNNs are widely used in applications such as time series forecasting, speech recognition, machine translation, and autonomous driving. The chapter outlines the main challenges of RNNs, including unstable gradients and limited short-term memory, and introduces advanced solutions such as LSTM, GRU, CNN-based sequence models, and WaveNet architectures.\n"
      ],
      "metadata": {
        "id": "Ow8SSR-lGOJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neurons and Layers**\n",
        "\n",
        "A recurrent neural network differs from a feedforward network because it has feedback connections. Each recurrent neuron receives not only the input at the current time step but also its own output from the previous time step. This feedback loop allows the network to maintain a form of memory.\n",
        "Mathematically, the output of a recurrent layer at time step t is computed using both the current input and the previous output. The same weights are reused at every time step, which enables the network to generalize across sequences of varying lengths. This process can be visualized by “unrolling” the network across time steps, turning it into a deep network where each layer represents a different time step.\n"
      ],
      "metadata": {
        "id": "p4bi3BnrGXT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memory Cells**\n",
        "\n",
        "The concept of a memory cell refers to any neural component that preserves information across time steps. In basic RNNs, the hidden state serves both as memory and output. However, this memory is short-lived and typically only captures patterns across a small number of time steps.\n",
        "Formally, a memory cell maintains a hidden state h(t), which depends on the current input x(t)and the previous state h(t-1). More advanced memory cells, such as LSTM and GRU, extend this idea by introducing mechanisms that selectively store, forget, and retrieve information.\n"
      ],
      "metadata": {
        "id": "Yy4tU2qwGcwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input and Output Sequences**\n",
        "\n",
        "RNNs can be configured to handle different types of sequence problems:\n",
        "•\tSequence-to-sequence: input and output are both sequences (e.g., time series prediction).\n",
        "•\tSequence-to-vector: input is a sequence, output is a single value (e.g., sentiment analysis).\n",
        "•\tVector-to-sequence: input is a single vector, output is a sequence (e.g., image captioning).\n",
        "•\tEncoder–Decoder: combines sequence-to-vector and vector-to-sequence architectures, commonly used in machine translation.\n",
        "Each architecture is suited to different real-world tasks depending on how input and output data are structured.\n"
      ],
      "metadata": {
        "id": "8hLJ_xyiGhal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training RNNs (Backpropagation Through Time)**\n",
        "\n",
        "RNNs are trained using Backpropagation Through Time (BPTT). The network is unrolled across time steps, and standard backpropagation is applied to the resulting deep network. Gradients flow backward through time, accumulating contributions from each time step.\n",
        "Because the same parameters are reused at each time step, gradients from all steps are summed together. While this approach is conceptually simple, it often leads to unstable gradients, especially for long sequences.\n"
      ],
      "metadata": {
        "id": "vSpCa0XbGlFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forecasting a Time Series**\n",
        "\n",
        "Time series forecasting is a common application of RNNs. In this chapter, a synthetic univariate time series is generated using sine waves and noise.\n"
      ],
      "metadata": {
        "id": "2_urKZFDGpzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "4qIe8KWoHKvV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
        "    return series[..., np.newaxis].astype(np.float32)\n"
      ],
      "metadata": {
        "id": "55Pca5mHGuSB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 50\n",
        "\n",
        "# Generate dataset\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "print(X_train.shape, y_train.shape)  # Cek bentuk data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXXsAWccG0JO",
        "outputId": "f3833eef-6eed-4e46-a7e3-c1c157b5471d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000, 50, 1) (7000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Models**\n",
        "\n",
        "Before using RNNs, baseline models are evaluated:\n",
        "•\tNaive forecasting, which predicts the last observed value.\n",
        "•\tLinear regression using a Dense layer, which significantly improves performance.\n",
        "These baselines are crucial for evaluating whether more complex models actually add value.\n"
      ],
      "metadata": {
        "id": "BkiS83_mHSXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing a Simple RNN**"
      ],
      "metadata": {
        "id": "IYUEasjKHVNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Buat model RNN sederhana\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "X8nadfipHYAr",
        "outputId": "419486d4-bc5c-4554-fe65-3af7052878c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep RNNs**\n",
        "\n",
        "Stacking multiple recurrent layers forms a Deep RNN, allowing the model to learn more complex temporal patterns.\n"
      ],
      "metadata": {
        "id": "hXLdSlwmHt2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1)\n",
        "])\n"
      ],
      "metadata": {
        "id": "_Dm28mlFHvzK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n"
      ],
      "metadata": {
        "id": "ANuTY8yIHfzm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forecasting Multiple Time Steps Ahead**\n",
        "\n",
        "Two strategies are discussed:\n",
        "1.\tPredicting one step at a time and feeding predictions back into the model.\n",
        "2.\tPredicting multiple future steps simultaneously.\n",
        "Sequence-to-sequence models provide better accuracy and training stability by producing outputs at every time step.\n"
      ],
      "metadata": {
        "id": "_KTyM6RcHz1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n"
      ],
      "metadata": {
        "id": "inqVgqn9H4rs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Long Sequences**\n",
        "\n",
        "Long sequences introduce two major problems:\n",
        "•\tUnstable gradients (exploding or vanishing).\n",
        "•\tShort-term memory limitations.\n",
        "Techniques such as gradient clipping, layer normalization, and dropout help stabilize training.\n"
      ],
      "metadata": {
        "id": "tYsclGSUH8Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization in RNNs**"
      ],
      "metadata": {
        "id": "AZPjAiMrH_TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]\n"
      ],
      "metadata": {
        "id": "dGvXLTWHICpK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Cells**\n",
        "\n",
        "Long Short-Term Memory (LSTM) cells solve the short-term memory problem by maintaining separate long-term and short-term states. Gates control what information is stored, forgotten, and output.\n"
      ],
      "metadata": {
        "id": "0F9C_zEfIHcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n"
      ],
      "metadata": {
        "id": "Rjmk-HrmILgk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Cells**\n",
        "\n",
        "GRU cells simplify LSTM architecture by combining gates and merging states, while achieving comparable performance.\n"
      ],
      "metadata": {
        "id": "blYqKFt3IPiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n"
      ],
      "metadata": {
        "id": "104NqrCGIOqP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using 1D CNNs for Sequences**\n",
        "\n",
        "1D convolutional layers can extract local temporal patterns and shorten sequences, making it easier for recurrent layers to learn long-term dependencies.\n"
      ],
      "metadata": {
        "id": "IZ6lyEb4IWoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLlHYRgUIZ1m",
        "outputId": "8ec97f91-3263-434b-f182-1b32918c1d12"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WaveNet Architecture**\n",
        "\n",
        "WaveNet uses dilated causal convolutions to efficiently model very long sequences without recurrence.\n"
      ],
      "metadata": {
        "id": "OtCtSnUAIcVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8G3knMnIgFL",
        "outputId": "f7cacd5e-589d-4e3e-9e16-1db6fb87261b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Chapter 15 demonstrates how RNNs and CNNs can be used to process sequential data effectively. While simple RNNs are useful for short sequences, advanced architectures like LSTM, GRU, and WaveNet are essential for handling long-term dependencies. Combining recurrent and convolutional layers often yields the best performance in real-world sequence modeling tasks.\n"
      ],
      "metadata": {
        "id": "ol_Nms9JIjt8"
      }
    }
  ]
}